services:
  koalawiki:
    image: crpi-j9ha7sxwhatgtvj4.cn-shenzhen.personal.cr.aliyuncs.com/koala-ai/koala-wiki
    environment:
      - KOALAWIKI_REPOSITORIES=/repositories
      - TASK_MAX_SIZE_PER_USER=5 # Maximum number of AI-processed document generations per user
      - REPAIR_MERMAID=1 # Whether to perform Mermaid repair, 1 for repair, others for no repair
      - CHAT_MODEL=gpt-4.1 # Must be a model that supports functions
      - ANALYSIS_MODEL=gpt-4.1 # Analysis model for generating repository directory structure, this is important, stronger models generate better directory structures, if empty uses ChatModel
                        # Analysis model recommended to use GPT-4.1, CHAT model can use other models to generate documents to save token costs
      - CHAT_API_KEY=YOUR_API_KEY_HERE # Your API key
      - LANGUAGE=English # Set generation language default to 'English', for Chinese can fill in Chinese or 中文
      - ENDPOINT=YOUR_OPENAI_ENDPOINT_HERE # OpenAI API endpoint
      - DB_TYPE=sqlite
      - DB_CONNECTION_STRING=Data Source=/data/KoalaWiki.db
      - UPDATE_INTERVAL=5 # Repository incremental update interval, in days
      - EnableSmartFilter=true # Whether to enable smart filtering, this may affect the file directory that AI gets from the repository
      - ENABLE_INCREMENTAL_UPDATE=true # Whether to enable incremental updates
      - ENABLE_CODED_DEPENDENCY_ANALYSIS=false # Whether to enable code dependency analysis? This may affect code quality.
      - ENABLE_WAREHOUSE_FUNCTION_PROMPT_TASK=true # Whether to enable MCP Prompt generation
      - ENABLE_WAREHOUSE_DESCRIPTION_TASK=true # Whether to enable repository Description generation
      - MAX_FILE_LIMIT=512 # Maximum file size limit in MB for uploads
    volumes:
      - ./repositories:/app/repositories
      - ./data:/data
    build:
      context: .
      dockerfile: src/KoalaWiki/Dockerfile
    network_mode: host
      
  koalawiki-web:
    image: crpi-j9ha7sxwhatgtvj4.cn-shenzhen.personal.cr.aliyuncs.com/koala-ai/koala-wiki-web
    environment:
      - NEXT_PUBLIC_API_URL=http://localhost:8090 # Address to provide to the client browser
      - PORT=3001 # Use different port to avoid conflicts
    build:
      context: ./web
      dockerfile: Dockerfile
    network_mode: host
      
  nginx: # Need nginx to proxy frontend and backend to one port
    image: crpi-j9ha7sxwhatgtvj4.cn-shenzhen.personal.cr.aliyuncs.com/koala-ai/nginx:alpine
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/conf.d/default.conf
    depends_on:
      - koalawiki
      - koalawiki-web
    network_mode: host 